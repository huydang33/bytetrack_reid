{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3301444-1e9e-4773-9366-e8d0bd1eabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOT17-02-FRCNN: 600 images\n",
      "600 ann images\n",
      "42 73\n",
      "MOT17-04-FRCNN: 1050 images\n",
      "1050 ann images\n",
      "104 140\n",
      "MOT17-05-FRCNN: 837 images\n",
      "837 ann images\n",
      "172 152\n",
      "MOT17-09-FRCNN: 525 images\n",
      "525 ann images\n",
      "189 43\n",
      "MOT17-10-FRCNN: 654 images\n",
      "654 ann images\n",
      "233 71\n",
      "MOT17-11-FRCNN: 900 images\n",
      "900 ann images\n",
      "274 89\n",
      "MOT17-13-FRCNN: 750 images\n",
      "750 ann images\n",
      "359 170\n",
      "loaded train_half for 2664 images and 58407 samples\n",
      "MOT17-02-FRCNN: 600 images\n",
      "600 ann images\n",
      "53 83\n",
      "MOT17-04-FRCNN: 1050 images\n",
      "1050 ann images\n",
      "122 141\n",
      "MOT17-05-FRCNN: 837 images\n",
      "837 ann images\n",
      "193 156\n",
      "MOT17-09-FRCNN: 525 images\n",
      "525 ann images\n",
      "215 29\n",
      "MOT17-10-FRCNN: 654 images\n",
      "654 ann images\n",
      "251 73\n",
      "MOT17-11-FRCNN: 900 images\n",
      "900 ann images\n",
      "295 91\n",
      "MOT17-13-FRCNN: 750 images\n",
      "750 ann images\n",
      "339 165\n",
      "loaded val_half for 2652 images and 53890 samples\n",
      "MOT17-02-FRCNN: 600 images\n",
      "600 ann images\n",
      "62 83\n",
      "MOT17-04-FRCNN: 1050 images\n",
      "1050 ann images\n",
      "145 141\n",
      "MOT17-05-FRCNN: 837 images\n",
      "837 ann images\n",
      "278 156\n",
      "MOT17-09-FRCNN: 525 images\n",
      "525 ann images\n",
      "304 43\n",
      "MOT17-10-FRCNN: 654 images\n",
      "654 ann images\n",
      "361 73\n",
      "MOT17-11-FRCNN: 900 images\n",
      "900 ann images\n",
      "436 91\n",
      "MOT17-13-FRCNN: 750 images\n",
      "750 ann images\n",
      "546 170\n",
      "loaded train for 5316 images and 112297 samples\n",
      "MOT17-01-DPM: 450 images\n",
      "0 -1\n",
      "MOT17-01-FRCNN: 450 images\n",
      "0 -1\n",
      "MOT17-01-SDP: 450 images\n",
      "0 -1\n",
      "MOT17-03-DPM: 1500 images\n",
      "0 -1\n",
      "MOT17-03-FRCNN: 1500 images\n",
      "0 -1\n",
      "MOT17-03-SDP: 1500 images\n",
      "0 -1\n",
      "MOT17-06-DPM: 1194 images\n",
      "0 -1\n",
      "MOT17-06-FRCNN: 1194 images\n",
      "0 -1\n",
      "MOT17-06-SDP: 1194 images\n",
      "0 -1\n",
      "MOT17-07-DPM: 500 images\n",
      "0 -1\n",
      "MOT17-07-FRCNN: 500 images\n",
      "0 -1\n",
      "MOT17-07-SDP: 500 images\n",
      "0 -1\n",
      "MOT17-08-DPM: 625 images\n",
      "0 -1\n",
      "MOT17-08-FRCNN: 625 images\n",
      "0 -1\n",
      "MOT17-08-SDP: 625 images\n",
      "0 -1\n",
      "MOT17-12-DPM: 900 images\n",
      "0 -1\n",
      "MOT17-12-FRCNN: 900 images\n",
      "0 -1\n",
      "MOT17-12-SDP: 900 images\n",
      "0 -1\n",
      "MOT17-14-DPM: 750 images\n",
      "0 -1\n",
      "MOT17-14-FRCNN: 750 images\n",
      "0 -1\n",
      "MOT17-14-SDP: 750 images\n",
      "0 -1\n",
      "loaded test for 17757 images and 0 samples\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/convert_mot17_to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413907d7-1856-46f7-bd32-8eb241701129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOT20-01: 429 images\n",
      "429 ann images\n",
      "55 78\n",
      "MOT20-02: 2782 images\n",
      "2782 ann images\n",
      "206 295\n",
      "MOT20-03: 2405 images\n",
      "2405 ann images\n",
      "515 715\n",
      "MOT20-05: 3315 images\n",
      "3315 ann images\n",
      "1241 1211\n",
      "loaded train_half for 4468 images and 519477 samples\n",
      "MOT20-01: 429 images\n",
      "429 ann images\n",
      "70 78\n",
      "MOT20-02: 2782 images\n",
      "2782 ann images\n",
      "244 189\n",
      "MOT20-03: 2405 images\n",
      "2405 ann images\n",
      "761 733\n",
      "MOT20-05: 3315 images\n",
      "3315 ann images\n",
      "1418 1209\n",
      "loaded val_half for 4463 images and 615137 samples\n",
      "MOT20-01: 429 images\n",
      "429 ann images\n",
      "74 78\n",
      "MOT20-02: 2782 images\n",
      "2782 ann images\n",
      "344 295\n",
      "MOT20-03: 2405 images\n",
      "2405 ann images\n",
      "1046 733\n",
      "MOT20-05: 3315 images\n",
      "3315 ann images\n",
      "2215 1211\n",
      "loaded train for 8931 images and 1134614 samples\n",
      "MOT20-04: 2080 images\n",
      "0 -1\n",
      "MOT20-06: 1008 images\n",
      "0 -1\n",
      "MOT20-07: 585 images\n",
      "0 -1\n",
      "MOT20-08: 806 images\n",
      "0 -1\n",
      "loaded test for 4479 images and 0 samples\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/convert_mot20_to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790a8771-a571-4938-887e-649a6ac3d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpath datasets/crowdhuman/annotation_val.odgt\n",
      "loaded val for 4370 images and 127716 samples\n",
      "fpath datasets/crowdhuman/annotation_train.odgt\n",
      "loaded train for 15000 images and 438792 samples\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/convert_crowdhuman_to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68fd78d-1a10-4be1-92ea-3774131e6e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mot17\n",
      "crowdhuman_train\n",
      "crowdhuman_val\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/mix_data_ablation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7d1ae1-33a3-4ef4-b3d9-a461e55c3d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train for 2056 images and 16720 samples\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/convert_ethz_to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17ba311-98c7-4c4b-b1f1-dd658e32bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/NCPC/Desktop/thesis/ByteTrack/tools/convert_cityperson_to_coco.py\", line 20, in <module>\n",
      "    os.mkdir(OUT_PATH)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'datasets/Cityscapes/annotations/'\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/convert_cityperson_to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2dcc35-b1b2-4b29-98fb-2e4fa335b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mot17\n",
      "crowdhuman_train\n",
      "crowdhuman_val\n",
      "ETHZ\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/NCPC/Desktop/thesis/ByteTrack/tools/mix_data_test_mot17.py\", line 122, in <module>\n",
      "    cp_json = json.load(open('datasets/Cityscapes/annotations/train.json','r'))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'datasets/Cityscapes/annotations/train.json'\n"
     ]
    }
   ],
   "source": [
    "!python3 tools/mix_data_test_mot17.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9a407-fc01-4572-b8d4-635dd78ab3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
